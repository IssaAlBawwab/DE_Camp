{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/03 06:47:26 WARN Utils: Your hostname, furix-Z690-UD-DDR4 resolves to a loopback address: 127.0.1.1; using 192.168.1.40 instead (on interface enp9s0)\n",
      "25/03/03 06:47:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/03 06:47:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"homework\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-03 07:47:55--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:2156:c200:b:20a5:b140:21, 2600:9000:2156:0:b:20a5:b140:21, 2600:9000:2156:f800:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:2156:c200:b:20a5:b140:21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  37.7MB/s    in 1.6s    \n",
      "\n",
      "2025-03-03 07:47:58 (37.7 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= spark.read.parquet('yellow_tripdata_2024-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-10-01 00:30:44|  2024-10-01 00:48:26|              1|          3.0|         1|                 N|         162|         246|           1|       18.4|  1.0|    0.5|       1.5|         0.0|                  1.0|        24.9|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:20|  2024-10-01 00:25:25|              1|          2.2|         1|                 N|          48|         236|           1|       14.2|  3.5|    0.5|       3.8|         0.0|                  1.0|        23.0|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:04:46|  2024-10-01 00:13:52|              1|          2.7|         1|                 N|         142|          24|           1|       13.5|  3.5|    0.5|       3.7|         0.0|                  1.0|        22.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:12:10|  2024-10-01 00:23:01|              1|          3.1|         1|                 N|         233|          75|           1|       14.2|  3.5|    0.5|       2.0|         0.0|                  1.0|        21.2|                 2.5|        0.0|\n",
      "|       1| 2024-10-01 00:30:22|  2024-10-01 00:30:39|              1|          0.0|         1|                 N|         262|         262|           3|        3.0|  3.5|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(4).write.parquet('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 90M\n",
      "-rw-r--r-- 1 furix furix 23M Mar  3 07:50 part-00000-2160ef99-5386-483a-90f2-7615e9a460ce-c000.snappy.parquet\n",
      "-rw-r--r-- 1 furix furix 23M Mar  3 07:50 part-00001-2160ef99-5386-483a-90f2-7615e9a460ce-c000.snappy.parquet\n",
      "-rw-r--r-- 1 furix furix 23M Mar  3 07:50 part-00002-2160ef99-5386-483a-90f2-7615e9a460ce-c000.snappy.parquet\n",
      "-rw-r--r-- 1 furix furix 23M Mar  3 07:50 part-00003-2160ef99-5386-483a-90f2-7615e9a460ce-c000.snappy.parquet\n",
      "-rw-r--r-- 1 furix furix   0 Mar  3 07:50 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|tpep_pickup_datetime| count|\n",
      "+--------------------+------+\n",
      "|          2024-10-15|128893|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, lit\n",
    "\n",
    "df.select(to_date(col(\"tpep_pickup_datetime\")).alias('tpep_pickup_datetime')) \\\n",
    "    .groupBy('tpep_pickup_datetime').count() \\\n",
    "    .filter(col(\"tpep_pickup_datetime\") == to_date(lit('2024-10-15'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|tpep_pickup_datetime| count|\n",
      "+--------------------+------+\n",
      "|          2024-10-15|128893|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT TO_DATE(tpep_pickup_datetime) AS tpep_pickup_datetime, COUNT(*) AS count\n",
    "    FROM df\n",
    "    GROUP BY TO_DATE(tpep_pickup_datetime)\n",
    "    HAVING tpep_pickup_datetime = '2024-10-15'\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max_duration_hours|\n",
      "+------------------+\n",
      "|162.61777777777777|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"\"\" \n",
    "          SELECT MAX((UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 3600) AS max_duration_hours\n",
    "          FROM df;\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-03 08:20:48--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:2156:8800:b:20a5:b140:21, 2600:9000:2156:7c00:b:20a5:b140:21, 2600:9000:2156:c600:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:2156:8800:b:20a5:b140:21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2025-03-03 08:20:49 (1.42 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv('taxi_zone_lookup.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+------------+\n",
      "|LocationID|Borough|          Zone|service_zone|\n",
      "+----------+-------+--------------+------------+\n",
      "|         1|    EWR|Newark Airport|         EWR|\n",
      "+----------+-------+--------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df_zones = df_zones.withColumn('LocationID',col('LocationID').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         pickup_zone|trip_count|\n",
      "+--------------------+----------+\n",
      "|Governor's Island...|         1|\n",
      "|       Rikers Island|         2|\n",
      "|       Arden Heights|         2|\n",
      "|         Jamaica Bay|         3|\n",
      "| Green-Wood Cemetery|         3|\n",
      "|Charleston/Totten...|         4|\n",
      "|   Rossville/Woodrow|         4|\n",
      "|Eltingville/Annad...|         4|\n",
      "|       West Brighton|         4|\n",
      "|       Port Richmond|         4|\n",
      "|         Great Kills|         6|\n",
      "|        Crotona Park|         6|\n",
      "|     Mariners Harbor|         7|\n",
      "|Heartland Village...|         7|\n",
      "|Saint George/New ...|         9|\n",
      "|             Oakwood|         9|\n",
      "|       Broad Channel|        10|\n",
      "|New Dorp/Midland ...|        10|\n",
      "|         Westerleigh|        12|\n",
      "|     Pelham Bay Park|        12|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"trips\")\n",
    "df_zones.createOrReplaceTempView(\"zones\")\n",
    "spark.sql(\"\"\"\n",
    "          SELECT zones.Zone as pickup_zone, count(*) as trip_count\n",
    "          FROM trips\n",
    "          left join zones on trips.PULocationID = zones.LocationID\n",
    "          GROUP BY pickup_zone\n",
    "          ORDER BY trip_count ASC\n",
    "          \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
